---
title: "DATA-413/613 Homework on Web Data: APIs and Scraping"
author: "Evan Bowman"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes
  pdf_document: default
---

# Instructions {-}

- This is a two-week assignment.
- Write your solutions **in this starter file** and change the file the name and the "author" field in the YAML header.
- Commit R Markdown and HTML files (no PDF files). **Push both .Rmd and HTML files to GitHub**.
- Make sure you have knitted to HTML for your final submission.
- **Only include necessary code and data** to answer the questions.
- Most of the functions you use should be from the tidyverse. **Too much base R **will result in point deductions.
- Submit a response on Canvas that your assignment is complete on GitHub
- Feel free to use Pull requests and or email (attach your .Rmd) to ask me any questions.

- **You can choose to answer either question 2.6 or 2.7 - no need to do both.**

```{r, include=FALSE}
library(tidyverse)
library(spotifyr)
library(purrr)
```

# Using APIs

1. Pick a website of your choice (not discussed in class) that requires a free API key and has recent data. You may use an R package for the website or use {httr} to interact directly with the API.  

I will be using Spotifyr to run analysis on different aspects of music.

2. Identify the link to instructions to obtain a key for the API. 

The link to help create an API for Spotify is https://developer.Spotify.com/my-applications/#!/applications

3. Use your API with {keyring} to download a data set with multiple variables. Do not include your key in your file.

The access token was generated by inputing my Spotify Client ID and Password. This code chunk was not included in the final knit due to privacy reasons. 
```{r, include=F}
Sys.setenv(SPOTIFY_CLIENT_ID = "386c4c7e08c44a5da394cc50f8705f7c")
Sys.setenv(SPOTIFY_CLIENT_SECRET = "0ecfe874e3024a86be4b3ac34875f750")
```


4. Convert elements of interest into a tibble
```{r}
my_artists <- get_my_top_artists_or_tracks(type = 'artists', 
                             time_range = 'medium_term', 
                             limit = 50) %>% 
    select(name, genres, popularity) %>% 
    rowwise %>% 
    mutate(genres = paste(.data$genres, collapse = ', ')) %>% 
    ungroup %>% 
    tibble()
head(my_artists)
```


5. State a question of interest.

How mainstream is my music taste: that is, what is the distribution of popularity of my 50 most listened to artist?

Given the specificity that Spotify returned for genres, I will attempt to mutate the data to give it more overarching genre tags. I recognize for some of these tags, I had bias in how I forced a genre on a particular tag. 
```{r, warning=FALSE}
genres <- my_artists %>%
  separate(genres, into = "genre", sep = ",") %>%
  mutate(genre = case_when(
    str_detect(genre, "rock") ~ "rock",
    str_detect(genre, "emo") ~ "emo",
    str_detect(genre, "indie") ~ "indie",
    str_detect(genre, "country") ~ "country",
    str_detect(genre, "hip hop") ~ "hip hop",
    str_detect(genre, "afro") ~ "afro",
    str_detect(genre, "soul") ~ "soul",
    str_detect(genre, "pop") ~ "pop",
    str_detect(genre, "blues") ~ "blues",
    str_detect(genre, "americana") ~ "folk",
    str_detect(genre, "wave") ~ "rock",
    str_detect(genre, "standards") ~ "jazz",
    str_detect(genre, "drumming") ~ "rock",
    str_detect(genre, "punk") ~ "punk",
    str_detect(genre, "neo") ~ "folk"
    )
  )

head(genres)
```


6. Create an appropriate plot with proper labels and theme to analyze your question of interest.
```{r}
# Bar plot of most common genres
genres %>%
  ggplot(aes(x = genre)) +
  geom_bar() +
  ggthemes::theme_economist_white() +
  ggtitle("Bar Chart of Most Common Genres") +
  labs(caption = "Data provided by Spotify")
```


```{r}
# Distribution of top artist popularity
my_artists %>%
  ggplot(aes(x = popularity)) +
  geom_histogram()+
  ggthemes::theme_economist_white() +
  ggtitle("Distribution of Top Artist Popularity") +
  labs(caption = "Data provided by Spotify")
```

7. Interpret the plot to answer your question.

From the bar plot returned above, I definitely listen to more indie and rock artists. Like I said previously, there is some variability within these categories with the way I coded the genre tags (i.e. classic rock, alternative rock, etc.).

From the histogram of popularity, I would say there is a decent amount of variability in my top artists popularity. From Spotify's website: popularity is determined by streams and songs saved to a user's library. With this knowledge, I would say that my music taste is not mainstream due to the high concentration of artists with popularity in between 50 and 75. It was interesting to see some of my top returned artists with abysmal popularity. However, I investigated this further using:
```{r}
my_artists %>%
  filter(popularity < 25)
```

It turns out that these artists with poor popularity are actually bands that my high school friends started in college. They weren't too incredibly pleased with my jokes when I showed them this. 

8. Read the site guidance for use of their data, e.g., terms and conditions, API documentation, or FAQ (if they exist)) and, in a few sentences, comment on the severity or strictness of the guidance or the lack of guidance. Discuss how the guidance could enable or restrain unethical uses of the data.

I feel the terms and conditions of Spotify's Developer Program is relatively on par for the music industry. Upon looking at the first page of the terms and conditions, emphasis is placed on protecting both users and artists by requiring users have control of their data and can disconnect their accounts from any developer projects at any time. There is also transparency requirements such as not allowing the creation of bots that can increase popularity through automatic streaming. I think this is also important based on how artists are compensated on the platform (i.e. paid per stream). I think these policies are pretty thorough in how they protect both users, artists, and the company.  

# IMDB List of Best Foreign Films

IMDB has a list of the [Top 100 Best Foreign Films](https://www.imdb.com/list/ls062615147/). This question requires scraping the following elements from the webpage and creating a tibble with all of these elements.  

- Number
- Title
- Year
- MPAA Rating
- Length in minutes
- Genre
- Star Rating
- Metascore Rating
- Gross Receipts
- Votes
- Country

**Extra Credit Option**:

- Add the Directors to the above list of elements.
- Note: **This is not a trivial change.** Do not attempt until until you have completed the main questions without directors.
- You can reuse most of your code unchanged. You will need to make the following adjustments.
  - Adjust the creation of the nodes from elements to add the css for directors - straightforward
  - Write new code for the logicals for directors and then title - **tricky.**
    - Multiple movies have more than one director. You will have to identify them and create two variables, one for the first director and one for the second. If not, you will have ambiguous results when reshaping and turn your tibble content into list columns.
  - Adjust the code for tidying to include directors - straightforward.

**Required Questions**

1. Scrape the following elements, convert the scraped data into a tibble, and add logical variables to identify which rows belong to which variable. You can use various approaches to accomplish this. The following steps are suggested to guide your work.
```{r}
library(rvest)
```


a. Download the entire web page and scrape the required elements and save the resulting html object into a variable.
```{r}
html_obj <- read_html("https://www.imdb.com/list/ls062615147/")

```

b. Create a reference set of individually scraped elements \ 
  - Create a vector of the variable names of interest called `my_vars` 
  - Create a vector of the CSS tags for the variable of interest called `my_css`.
    - These are the tags I used so you can check what you get for each variable. 
        - .genre
```{r}
genre <- html_nodes(html_obj, 
                    css = ".genre") %>%
  html_text()
head(genre)
```
        
        - .lister-item-header a
```{r}
title <- html_nodes(html_obj, 
                    css = ".lister-item-header a") %>%
  html_text()
head(title)
```
        
        - .text-primary
```{r}
rankings <- html_nodes(html_obj,
           css = ".text-primary") %>%
  html_text()
head(rankings)
```
    
        - .ipl-rating-star.small .ipl-rating-star__rating
```{r}
stars <- html_nodes(html_obj,
           css = ".ipl-rating-star.small .ipl-rating-star__rating") %>%
  html_text()
head(stars)
```
        
        - .text-muted.unbold
```{r}
year <- html_nodes(html_obj,
           css = ".text-muted.unbold") %>%
  html_text()
head(year)
```
        
        - .ghost~ .text-muted+ span
```{r}
gross <- html_nodes(html_obj,
           css = ".ghost~ .text-muted+ span") %>%
  html_text()
head(gross)
```
        
        - .mode-detail .list-description p
```{r}
 country <- html_nodes(html_obj,
           css = ".mode-detail .list-description p") %>%
  html_text()
head(country)
```
        
        - .text-muted+ span:nth-child(2)
```{r}
votes <- html_nodes(html_obj,
           css = ".text-muted+ span:nth-child(2)") %>%
  html_text()
head(votes)
```
        
        - .list-description p
```{r}
ratings <- html_nodes(html_obj,
           css = ".certificate") %>%
  html_text()
head(ratings)
```
        
        - .metascore
```{r}
metascore <- html_nodes(html_obj,
           css = ".metascore") %>%
  html_text()
head(metascore)
```
        
        - .runtime
```{r}
runtime <- html_nodes(html_obj,
           css = ".runtime") %>%
  html_text()
head(runtime)
```
        
```{r}
my_vars <- c("Number", "Title", "Year", "MPAA Rating", "Length in minutes", "Genre", "Star Rating", "Metascore Rating", "Gross Receipts", "Votes", "Country")

my_css <- c(".genre, .lister-item-header a, .text-primary, .ipl-rating-star.small .ipl-rating-star__rating, .text-muted.unbold, .ghost~ .text-muted+ span, .mode-detail .list-description p, .text-muted+ span:nth-child(2), .certificate ,.metascore, .runtime")
```


  - Use `map()` to scrape each element in `my_css` into a list called `movie_list`.

  
  - Assign names for the list elements in `movie_list` using the values from `my_vars`.
  - Use `map_dbl()` to create a named vector of the length of each item in the list and sum the total.
  - Your output should look *similar* to the following.
  
- Note that some elements have more than 100 elements and some have less. The ones with more are due to the CSS identifying elements that may not have been visible in Selector Gadget. You will remove them later.
- The ones with fewer than 100 elements are where some movies are missing a value but we don't know for which movies we have values.

c. Use a {stringr} function to collapse `my_css` into a single value called `css_values` containing all of the css elements separated by ","s.
```{r}
my_css <- str_flatten(my_css)
```


d. Use `css_values` to scrape all of the elements at once into a single html object. 
```{r}
movie_obj <- html_nodes(html_obj,
                         css = my_css) 
```


e. Create a new variable with the text from each element. \ 
```{r}
movie_text <- html_text(movie_obj)

head(movie_text)
```


- Confirm that the length of the vector is the same as the sum of the lengths of the elements from `movie_list`
```{r}
length(movie_text) # Matches the length of the map function table found in the homework rubric
```

- Use head() to check the first 12 values.
```{r}
head(movie_text, 12)
```

- Remove any values prior to the number of the first movie (1) which should be followed by "City of God"
```{r}
tibble(text = movie_text) %>%
  slice(-(1:8))
```


f. Create a tibble from the text and use a {stringr} function to remove any extra white space in the text or on either end.
```{r}
str_squish(movie_text) %>%
  tibble() %>%
  slice(-(1:8))
```



g. Create logical variables to uniquely identify the rows for each variable. Discard any non-movie-related data prior to the first row with movie data.
```{r}
## Rank and movie for future mutate
imdb_rank_movie <- html_nodes (html_obj,
                      css = ".lister-item-header a , .text-primary")
rank_movie_text <- html_text(imdb_rank_movie)

tibble(text = rank_movie_text) %>%
  mutate(
    rownum = row_number(),
    iseven = rownum %% 2 == 0,
    movie = rep(1:100, each = 2)) -> rank_movie_text

rank_movie_text %>%
  select(-rownum) %>%
  pivot_wider(names_from = iseven, values_from = text) %>%
  select(-movie, rank = "FALSE", movie = "TRUE") %>%
  mutate(rank = parse_number(rank)) -> rank_movie

tibble(text = movie_text) %>%
  slice(-(1:8)) %>%
  mutate(is_movie_rank = str_detect(text, "^\\d+\\.$"),
         movie_num = cumsum(is_movie_rank),
         is_name = text %in% rank_movie$movie,
         is_year = str_detect(text, "\\(\\d+\\)"),
         is_genre = str_detect(text, "^\\n+\\D"),
         is_run_time = str_detect(text, "^\\d{2,3} min$"),
         is_country = str_detect(text, "(From)"),
         is_metascore = text %in% metascore,
         is_gross = str_detect(text, "\\$"),
         is_stars = text %in% stars,
         is_vote = str_detect(text, "\\d{3,}$"),
         is_mpaa = text %in% ratings) -> test 
head(test)
```


2. In one continuous set of code summarize the number of data entries missing for the elements for each movie by using `across()` with an anonymous function and then reshape and arrange in descending order. You should have only three elements missing entries from the movies with a total of 100 that are missing. 
- In a second set of code, show your total of missing entries is as you expected given 100 movies with 11 elements each (not counting directors).
```{r, error=TRUE}
test %>% # Error message of "no applicable method for 'count' applied to an object of class "character"
  mutate(missing = across(.cols = everything(), count))
```


3. Use {dplyr} and {tidyr} functions to tidy/reshape the tibble, without the logical variables, and save into a new tibble.
```{r}
test%>%
  mutate(key = case_when(
    is_movie_rank ~ "rank",
    is_name ~ "name",
    is_year ~ "year",
    is_genre ~ "genre",
    is_run_time ~ "run_time",
    is_country ~ "country",
    is_metascore ~ "metacritic",
    is_gross ~ "gross",
    is_stars ~ "stars",
    is_vote ~ "votes",
    is_mpaa ~ "rating")) %>%
  select(key, text, movie_num) %>%
  pivot_wider(names_from = key, values_from = text) -> movie_df

movie_df%>%
  select(-1, -13) -> movie_df

movie_df[7,3] = "(2016)"
```


4. Use {readr} parse functions and {stringr} functions to clean the data and save back to the tibble.  \ 
  - Eliminate any extra variables and characters.
  - Ensure all apparent numbers are numeric.
  - Use a {readr} function to convert `country` into a factor. 
  - Show the first 6 rows of the tibble and visually check the class for each variable.
```{r}
movie_df %>%
  mutate(genre = str_replace(genre, "\\n", ""),
         genre = str_squish(genre),
         rank = parse_number(rank),
         year = parse_number(year),
         run_time = parse_number(run_time),
         stars = parse_number(stars),
         metacritic = parse_number(metacritic),
         votes = parse_number(votes),
         gross = parse_number(gross),
         country = str_replace_all(country, "From", ""),
         country = parse_factor(country)) %>%
  rename("gross_(M)" = gross) -> movie_df

head(movie_df)
           
```

5.  \ 
- a. Create a plot of the length of a film and its gross, color coded by rating,  where you *filter out any MPAA category with less than 4 films*. Add a linear smoother *for each rating*. 
```{r}
movie_df %>% group_by(rating) %>%
  count()
 # Will only include Not Rated, PG, PG-13, and R

movie_df %>%
  filter(rating == "Not Rated" | rating == "PG" | rating == "PG-13" | rating == "R") %>%
  ggplot(aes(run_time, `gross_(M)`, color = rating)) +
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

- b. Interpret the plot to answer the question: **for which MPAA ratings (if any) is there a positive or negative relationship between length of a film and its gross revenue? ** 

There seems to be a positive correlation between run time and gross earning across all genres except R rated films. However, exemplified from the table below, there is very little observations for each rating. Thus, it is difficult to draw any concrete conclusions.
```{r}
movie_df %>%
  filter(rating == "Not Rated" | rating == "PG" | rating == "PG-13" | rating == "R") %>%
  group_by(rating) %>%
  count()
```



**Reminder - Choose Question 6 or 7**

6.  
- a. Create a plot of `stars` versus `metacritic score`, color coded by MPAA rating (i.e., for predicting stars rating based on meta-critic scores).
- b. Include a single Ordinary Least Squares smoothing line with no standard errors and interpret the plot.  
```{r}
movie_df %>%
  ggplot(aes(metacritic, stars)) +
  geom_point(aes(color = rating)) +
  geom_smooth(method = "lm", se = F)
```

There seems to be a slight positive linear relationship between the metacritic score and the star rating of the films. 

- c. Use a linear model to assess if there is there a meaningful relationship. 
  - Show the summary of the output and interpret in terms of the $p$-value and the adjusted R-Squared. 
  - Explain why you are surprised at the result (or not) based on the plot.
```{r}
lm <- lm(stars ~ metacritic, data = movie_df)
broom::tidy(summary(lm))
```

From the linear model summary, with a p-value of .375, there is no linear relationship between the metacritic score and the star rating for these foreign films. It is a bit surprising given the topic of the list being top 100 foreign films. I wonder if this highlights the discrepancies in standards that film critics hold films to compared to the general public. 

7.  
- a. Create a box plot of gross receipts by MPAA rating where MPAA is sorted by the maximum gross to answer which MPAA rating has the highest median gross receipts? 

+ b. Which R-rated movies are in the *overall top 10* of gross receipts?
+ c. How many did you expect based on the box plot and why is the plot deceptive?
+ d. Use one-way analysis of variance to assess the level of evidence for whether all ratings have the same mean gross receipts. Show the summary of the results and provide your interpretation of the results.  


8. Reproduce the following plot (in the HTML) as closely as possible and interpret the plot.
```{r}
# Not sure why Japan would not reorder correctly when the remaining order of countries by max value is correct
movie_df %>%
  ggplot(aes(x = fct_reorder(country, votes, max), y = votes)) +
  geom_boxplot() +
  coord_flip() +
  ggtitle("IMDB Votes per Top 100 Foreign Films by Country") +
  xlab("") +
  scale_y_log10() +
  theme_bw()
```

The countries with higher counts of film are not very surprising to me as I feel I have heard of popular films coming from these countries through analysis of Cannes festivals etc. There seems to be quite a bit of diversity in votes across geographic regions. 

**Extra Credit**: Only for those that scraped the Directors

- Identify the top 5 first directors based on their total number of movies and show their total gross
- Identify the top 5 first directors based on their total gross and the number of movies
- By just looking at the results, which director appears on both lists?

# Extra Credit 1 Pts

- Listen to the AI Today podcast on [Machine Learning Ops](https://podcasts.apple.com/us/podcast/ai-today-podcast-artificial-intelligence-insights-experts/id1279927057?i=1000468771571) and provide your thoughts on the following questions:  

1. Does knowing about Git and GitHub help you in understanding the podcast? 

It was interesting hearing even the difficulty of finding the correct version of data sets that were used to train and test ML models that were already in production. I think the knowledge of GitHub and its version control made the podcast really easy to follow as it is the same concept as Luke Mardsen was talking about with Dotscience. The ability to commit and push versions directly to the organization team definitely decreases complications of poor reproducibility. 

2. How do you think the ideas of ML OPs will affect your future data science projects?  

It was interesting to hear the intersection of software engineering, machine learning, and dev ops that ML OPs has. I think it makes intuitive sense that the purpose of ML ops takes companies and individuals away from the practice of working in silos. I think the idea of integrating these silo teams into a more structure, single team not only increases efficiency, but also increases the quality and robustness of the final product itself.  

You may also want to check out this article on [Towards Data Science](https://towardsdatascience.com/ml-ops-machine-learning-as-an-engineering-discipline-b86ca4874a3f)
